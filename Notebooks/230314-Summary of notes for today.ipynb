{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My other notebook became filled with test code, so I made a new one for today to keep benchmarking notes close together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To practice writing performant Julia code I am attempting to improve the speed of part 1. As a reminder, the terminal output after the last run was:\n",
    "\n",
    "        Processing folder: info\n",
    "        Finished in 4.35 seconds\n",
    "        Processing folder: local-monthly-gross-returns\n",
    "        Finished in 130.5 seconds\n",
    "        Processing folder: local-monthly-net-returns\n",
    "        Finished in 148.06 seconds\n",
    "        Processing folder: monthly-costs\n",
    "        Finished in 63.1 seconds\n",
    "        Processing folder: monthly-morningstar-category\n",
    "        Finished in 467.8 seconds\n",
    "        Processing folder: monthly-net-assets\n",
    "        Finished in 237.36 seconds\n",
    "        Processing folder: usd-monthly-gross-returns\n",
    "        Finished in 85.24 seconds\n",
    "        Processing folder: usd-monthly-net-returns\n",
    "        Finished in 100.15 seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total runtime of the script (missing the time taken to load the info dataframe initially), then is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.61 minutes\n"
     ]
    }
   ],
   "source": [
    "println(round((4.35+130.5+148.06+63.1+467.8+237.36+85.24+100.15)/60, digits=2), \" minutes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first change made is to employ parallel processing through @threads. The new terminal output is:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I lost the terminal output after the first run because I had to stop it partway through, but had recorded two of the times manually. It looked something like:\n",
    "\n",
    "        Regrouping files...\n",
    "        Processed folder info in 12.6 seconds\n",
    "        Processed folder monthly-costs in 1412.8 seconds\n",
    "        Processed folder monthly-usd-gross-returns in 1730.4 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monthly-costs took 23.55 minutes\n",
      "monthly-usd-gross-returns took 28.84 minutes\n"
     ]
    }
   ],
   "source": [
    "println(\"monthly-costs took $(round(1412.8/60, digits=2)) minutes\")\n",
    "println(\"monthly-usd-gross-returns took $(round(1730.4/60, digits=2)) minutes\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of time for usd-gross-returns to time for monthly-costs remained roughly the same, but took 22x as long, so there was no benefit to parallelism at all and the overtime time taken was substantially increased."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a measure of the speed-up obtained simply by allowing CSV.read to run multithreaded, here is the terminal output from running the original code with number of threads increased to auto in settings:\n",
    "\n",
    "        Processed folder info in 4.35 seconds\n",
    "        Processed folder local-monthly-gross-returns in 109.61 seconds\n",
    "        Processed folder local-monthly-net-returns in 142.17 seconds\n",
    "        Processed folder monthly-costs in 59.3 seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stopped the code after getting the first three examples. Looks like between 5-20 seconds was saved by utiliting multithreading."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote a new script to clean all the csv files to make future loads faster, but it took almost as long (11 minutes total) and would not likely save more than that amount of time in the next step. Terminal output:\n",
    "\n",
    "        Cleaning csv files...\n",
    "        Finished cleaning info in 15.46 seconds.\n",
    "        Finished cleaning local-monthly-gross-returns in 141.7 seconds.\n",
    "        Finished cleaning local-monthly-net-returns in 149.93 seconds.\n",
    "        Finished cleaning monthly-costs in 78.25 seconds.\n",
    "        Finished cleaning monthly-morningstar-category in 132.58 seconds.\n",
    "        Finished cleaning monthly-net-assets in 294.68 seconds.\n",
    "        Finished cleaning usd-monthly-gross-returns in 135.9 seconds.\n",
    "        Finished cleaning usd-monthly-net-returns in 175.42 seconds.\n",
    "        Finished cleaning all csv files in 1123.93 seconds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to clean the files directly without using CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
